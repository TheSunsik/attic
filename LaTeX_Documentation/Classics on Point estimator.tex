\documentclass[10pt]{article}
\usepackage[utf8]{inputenc}
\usepackage{fullpage}
\usepackage{paralist}
\usepackage{amsfonts}

\def\estimator {}
\def\parameter {}
\def\score {}
\def\scoresq {}

\title{%
Classics on Point estimator \\
\large{From Casella\ \&\ Berger textbook}}
\author{Sunsik Kim}
\date{}

\begin{document}
\maketitle

\section{Cramer-Rao lower bound}
MSE of a $T(\textbf{X})$ subject to the $g(\theta)$ can be partitioned into bias and variance of the estimator. Therefore, we look for the estimator whose bias and variance are both low. Classical approach looks for the estimator with lowest variance among the set of unbiased estimators to find best estimator of $g(\theta)$. There exists two main approach for this work. 

\noindent First of the two is CRLB. This theory provides the lower bound of variance of an unbiased estimator of $g(\theta)$. Therefore, if the variance of an unbiased estimator $T(\textbf{X})$ equals to the lower bound provided  by this theorem, then we can conclude that $T(\textbf{X})$ is the best unbiased estimator of $g(\theta)$(i.e. MVUE of $g(\theta)$).

\subsection{Fisher information}
\textbf{Score function}\ \ \ The score is the gradient of the logarithm of the likelihood function with respect to some parameter $\theta$. That is, we define score function $V(\theta, \textbf{X})$ as:
$$
V(\theta, \textbf{X})=\displaystyle\frac{\partial}{\partial\theta}\log L(\theta;\textbf{X})
$$
The score indicates the sensitivity of likelihood function with respect to $\theta$.\bigskip

\noindent \textbf{Fisher information}\ \ \ We define Fisher information $I(\theta)$ as the variance of score function. By applying simple mathematics, we can show:
$$
I(\theta)=Var\displaystyle\frac{\partial}{\partial\theta}\log L(\theta;\textbf{X})=-E\displaystyle\frac{\partial^2}{\partial\theta^2}\log L(\theta;\textbf{X})
$$

\subsection{Cramer-Rao lower bound(CRLB)}
This theorem shows that any unbiased estimator $T(\textbf{X})$ of $g(\theta)$ satisfies:
$$
VarT(\textbf{X})\geq\frac{\{g'(\theta)\}^2}{I(\theta)}
$$
(Note that when $\textbf{X}=\{X_1,...,X_n\}$ is an iid sample, $I(\theta)=-E\displaystyle\frac{\partial^2}{\partial\theta^2}\log L(\theta;\textbf{X})=-nE\displaystyle\frac{\partial^2}{\partial\theta^2}\log f(x;\theta)
$). Corollary below gives convenient way to attain the MVUE of $g(\theta)$ based on CRLB.\bigskip

\textbf{Corollary 7.3.15 (Attainment)} If $T(\textbf{X})$ is any unbiased estimator of $g(\theta)$, then $T(\textbf{X})$ attains the CRLB if and only if
$$
\displaystyle \frac{\partial}{\partial\theta}\log L(\theta;\textbf{X})=h(\theta)[T(\textbf{X})-g(\theta)]
$$
for some function $h(\theta)$.\bigskip

\noindent However, non-existence of an estimator whose variance touches CRLB doesn't mean that MVUE of $g(\theta)$ does not exist. In such case, we use below theorem to get it.

\section{Lehmann-Scheffe's Theorem}
This theorem uses complete-sufficient statistic(CSS) to obtain MVUE of $g(\theta)$, and it heavily depends on Rao-Blackwell theorem which invented new form of statistic which utilizes sufficient statistic.

\subsection{Rao-Blackwell's Theorem}
For any unbiased estimator $T(\textbf{X})$ of $g(\theta)$, let $S$ be a sufficient statistic of $f(x;\theta)$. To Rao-Blackwellize a statistic $T(\textbf{X})$ is to obtain expectation of $T(\textbf{X})|S$. Denote Rao-Blackwellized statistic as: $$\delta(S)=E_{\theta}[\ T(\textbf{X})|S\ ]$$ Then Rao-Blackwell theorem states that this new statistic has smaller variance than the original one, while maintaining unbiasedness.
\begin{itemize}
\item $E\delta(S)=EE[\ T(\textbf{X})|S\ ]=ET(\textbf{X})=g(\theta))$
\item $Var(T(\textbf{X}))=E(T(\textbf{X})-E\delta(S))^2=E(T(\textbf{X})-\delta(S)+\delta(S)-E\delta(S))^2=E(T(\textbf{X})-\delta(S))^2+Var(\delta(S))$
\end{itemize}

\subsection{Lehmann-Scheffe's Theorem}
To summarize above content, we can obtain an unbiased estimator $\delta(S)$ using original unbiased estimator $T(\textbf{X})$ by conditioning sufficient statistic $S$ on $T(\textbf{X})$. However, if $S$ is not only sufficient but also complete(i.e. $S$ is CSS), then this Rao-Blackwellized statistic is MVUE of $g(\theta)$ by Lehmann-Scheffe's theorem.

\end{document}

