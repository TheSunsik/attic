\documentclass[10pt]{article}
\usepackage[utf8]{inputenc}
\usepackage{fullpage}
\usepackage{paralist}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{mathabx}

\title{%
Sampling Distribution \\
\large{From Casella\ \&\ Berger textbook}}
\author{Sunsik Kim}
\date{}

\begin{document}
\maketitle
\noindent When a sample $X_1,...,X_n$ is drawn, some summary of the values is usually computed. Such summary can be understood as output of a function $T:(X_1,...,X_n) \rightarrow Y$, and we call the random variable(or vector) $Y=T(X_1,...,X_n)$ as \textit{statistic}. Since the random sample $X_1,...,X_n$ has a simple probabilistic structure, the distribution of $Y$ is particularly tractable. We call that distribution as \textit{sampling distribution of $Y$}. In this chapter, sampling distribution of two types of statistic are covered. Then the concept on Convergence will be explained, which enables the user to track asymptotic sampling distribution of certain statistic.

\section{Sums of Random Variables}
$\overline{X}, S^2$ are often used and provide good summaries of the sample. Regardless of the distribution where this samples are drawn, these statistics have nice properties. Let $X_1,...,X_n$ be a random sample from a population with mean $\mu$ and variance $\sigma^2<\infty$. Then:
\begin{itemize}
    \item $E\overline{X}=\mu$
    \item $Var\overline{X}=\sigma^2/n$ 
    \item $ES^2=\sigma^2$
\end{itemize}
We now discuss ways to obtain the distribution of $\overline{X}$. Let $Y=X_1+\cdots +X_n$.
\begin{enumerate}
    \item $f_{\overline{X}}(x)=nf_Y(nx)$
    \item $M_{\overline{X}}(t)=Ee^{t\overline{X}}=Ee^{t(X_1+\cdots +X_n)/n}=M_Y(t/n)$
\end{enumerate}
Additionally, if we are sampling from a location-scale family, the sampling distribution $\overline{X}$ is easy to derive. Suppose $X_1,...,X_n$ is a random sample from $(1/\sigma)f((x-\mu)/\sigma)$, a member of a location-scale family. Let $X_i=\sigma Z_i+\mu$. Then $\overline{X}$ and $\overline{Z}$ are related by $\overline{X}=\sigma\overline{Z}+\mu$. Accordingly, if $g(z)$ is the pdf of $\overline{Z}$, then

$$
f_{\overline{X}}(x)=\displaystyle \frac{1}{\sigma}g_{\overline{Z}}(\frac{x-\mu}{\sigma})
$$

\noindent is the pdf of $\overline{X}$. This is useful if it is easier to work first with $Z_1,...,Z_n$ and then get the form of $f_{\overline{X}}(x)$.

\section{Order Statistics}
An i-th order statistic $X_{(i)}$ is a function from $(X_1,...,X_n)$ to the i-th smallest sample. First, we can obtain joint pdf of $X_{(1)},...,X_{(n)}$ using transformation concept. For better illustration, consider the case when n=2 where the support of a sample $X_i(i=1,2)$ is $A=(0,1)$(i.e. support of $(X_1, X_2)$ is $A^2=(0,1)\bigtimes (0,1)$). This square area can be divided into 2! right isosceles triangle of $A_1=\{(X_1, X_2):X_1>X_2\}$ and $A_2=\{(X_1, X_2):X_1<X_2\}$. In $A_1$, $X_{(1)}=X_2$ and $X_{(2)}=X_1$ while opposite holds in $A_2$. This result in:

$$
f_{X_{(1)}, X_{(2)}}=f_{X_2, X_1}(x_2,x_1)|-1|+f_{X_1, X_2}(x_1,x_2)|1|=\displaystyle \sum_{i=1}^{2!}f_{X_1, X_2}(x_1,x_2)
$$

\noindent In similar sense, joint pdf of $X_{(1)},...,X_{(n)}$ is obtained as:

$$
f_{X_{(1)},...,X_{(n)}}(x_1,...x_n)=\displaystyle \sum_{i=1}^{n!}f_{X_1,...,X_n}(x_1,...,x_n)=n!f_X(x_1)\cdots f_X(x_n)
$$

\noindent Starting from joint pdf above, we can integrate out unnecessary variables to obtain desired pdf on order statistic(s). First, final form of joint pdf of $X_{(j)}, X_{(k)}$ becomes:

$$
{n\choose j\ k-j\ n-k}\times j\times (k-j)\times [F_x(x_j)]^{j-1}[F_x(x_k)-F_x(x_j)]^{k-j-1}[1-F_x(x_k)]^{n-k}f_X(x_j)f_X(x_k)
$$

\noindent while marginal pdf of $X_{(k)}$ becomes:

$$
{n \choose k}\times k\times[F_X(x_k)]^{k-1}[1-F_X(x_k)]^{n-k}f_X(x_k)
$$

\noindent Above result on marginal distribution of $X_{(k)}$ can be heuristically understood as calculating probability for $X_{(k)}=x$. Among n samples, for $X_{(k)}$ equal $x$, $P(X \leq x)$ and $P(X > x)$ should occur $k, (n-k)$ times respectively. For that, choose $k$ samples among n samples(i.e. n choose k) and pick one sample from that k chosen samples to be labeled as $X_{(k)}$(i.e. k choose 1).

\section{Converge in Distribution}
This section covers the limiting behaviour or distribution of certain sample quantities. First, definition on \textit{convergence in distribution} is presented:\bigskip

\noindent\textbf{Definition 5.5.10} A sequence of random variables $X_1,X_2...$ \textit{converges in distribution} to a random variable $X$ if following holds for all points $x$ where $F_X(x)$ is continuous.
$$
\lim_{n\rightarrow \infty}F_{X_n}(x)=F_X(x)
$$

\subsection{Central Limit Theorem(CLT)}
The sample mean is one statistic whose large-sample behavior is both important and well-known. Limiting distribution of sample mean $\overline{X}_n=(1/n)\sum_{i=1}^{n}X_i$ is well-known based on the most fundamental theorem of Statistics, CLT:\bigskip

\noindent\textbf{Theorem 5.5.14(CLT)} Let $X_1, X_2, ...$ be a sequence of iid random variables where $M_{X_i}(t)$ exists for $|t|<h$. Let $EX_i=\mu$ and $Var X_i = \sigma^2<0$. Finally, let $G_n(x)$ denote the cdf of $\sqrt{n}(\overline{X}_n-\mu)/\sigma$. Then,
$$
\displaystyle \lim_{n\rightarrow \infty}G_n(x)=\int_{-\infty}^{x}\frac{1}{\sqrt{2\pi}}e^{-y^2/2}dy
$$
for any real number x(i.e. $\sqrt{n}(\overline{X}_n-\mu)/\sigma$ \textbf{ has a limiting standard normal distribution}).

\subsection{Slutsky's Theorem}
There are some cases where a user can track limiting distribution of product or sum of random variables by applying one simple theorem; Slutsky's Theorem. For the understanding of the necessary condition of the theorem, knowledge on \textit{convergence in probability} must be preceded.\bigskip

\noindent This is one formal way of saying that a sequence of random variables $\{X_n\}$ is getting close to another random variable or certain constant as $n\rightarrow\infty$. Formal definition follows:\bigskip

\noindent \textbf{Definition 5.5.1} A sequence of random variable $\{X_n\}$ \textit{converges in probability} to a random variable $X$ if

$$
\displaystyle\lim_{n\rightarrow\infty}P(|X_n-X|\geq\varepsilon)=0\quad \forall\varepsilon>0
$$

\noindent Typical way of showing convergence in probability is using \textit{Chebyshev's inequality}. According to this inequality, if mean and variance of a random variable $X$ are $\mu$ and $\sigma^2$ respectively and are finite, then 

$$
\displaystyle P(|X-\mu|\geq k\sigma)\leq \frac{1}{k^2}
$$

\noindent For example, one can easily prove \textit{Weak Law of Large Numbers(WLLN)} using this inequality, which states that $\overline{X}_n$ \textit{converges in probability to} $\mu$.

$$
\displaystyle 0\leq\lim_{n\rightarrow\infty}P(|\overline{X}_n-\mu|\geq\epsilon)=\lim_{n\rightarrow\infty}P(|\overline{X}_n-\mu|\geq\frac{\sigma}{\sqrt{n}}\times\frac{\sqrt{n}}{\sigma}\varepsilon)\leq\lim_{n\rightarrow\infty}\frac{\sigma^2\varepsilon^2}{n}=0
$$

\noindent Note that \textit{convergence in probability} is closed under linear combination. That is

$$
X_n \overset{p}{\to} X, Y_n \overset{p}{\to} Y \Rightarrow aX_n+bY_n \overset{p}{\to} aX+bY
$$

\noindent Moreover, if $g$ is a continuous function, $X_n \overset{p}{\to} X\Rightarrow g(X_n) \overset{p}{\to} g(X)$ holds. This means that, given $Y_n/n \overset{p}{\to} p$, $1-Y_n/n \overset{p}{\to} 1-p$ or $(Y_n/n)(1-Y_n/n) \overset{p}{\to} p(1-p)$ holds. Now, Slutsky's theorem can be presented using predefined concepts:\bigskip

\noindent \textbf{Theorem 5.5.17 (Slutsky's Theorem)} If $X_n \overset{d}{\to} X$ and $Y_n \overset{p}{\to} a$, then:
\begin{itemize}
    \item $Y_nX_n \overset{d}{\to} aX$
    \item $X_n+Y_n \overset{d}{\to} X+a$
\end{itemize}


\subsection{Delta Method}
This method will help a user which is not specifically interested in the limiting distribution of the random variable itself, but rather in some function of the random variable $g(X_n)$. Consider Taylor expansion of $g(X_n)$ at $\theta$:

$$
g(X_n)=g(\theta)+g'(\theta)(X_n-\theta)+\displaystyle \sum_{r=2}^{\infty}\frac{g^{(r)}(X_n)}{r!}(X_n-\theta)^r
$$

\noindent Define \textit{little-o} expression as following:

\begin{center}
    $a=o(b)$ iff  $b \rightarrow 0 \Rightarrow \frac{a}{b} \rightarrow 0$
\end{center}

\noindent Let $R(X_n)=\sum_{r=2}^{\infty}\frac{g^{(r)}(X_n)}{r!}(X_n-\theta)^r$, then since $|X_n-\theta|\rightarrow 0\Rightarrow \frac{R(X_n)}{|X_n-\theta|}\rightarrow 0$, rewrite above expansion as:

$$
g(X_n)=g(\theta)+g'(\theta)(X_n-\theta)+o(|X_n-\theta|)
$$

\noindent To complete the justification of this method, we need additional concept on \textit{bounded in probability} and corresponding theorems. Index of the following statements are based on Hogg \& Craig's book.\bigskip

\noindent \textbf{Definition 5.2.2} If following holds, we say that $\{X_n\}$ is bounded in probability:
$$
\forall \varepsilon>0,\ \exists N\in \mathbb{N}, B>0\ s.t.\ n\geq N \Rightarrow P[|X_n|\leq B]\geq 1-\varepsilon
$$

\noindent \textbf{Theorem 5.2.6.} $X_n \overset{d}{\to} X \Rightarrow \{X_n\}$ is bounded in probability

\noindent \textbf{Theorem 5.2.8.} $\{X_n\}$ is bounded in probability $\Rightarrow Y_n=o(X_n) \overset{p}{\to}0$\bigskip

\noindent With these conceptual background, theorem on Delta method is presented as below:\bigskip

\noindent \textbf{Theorem 5.2.9.(Delta method)} Let $g(x)$ is differentiable at $\theta$ and $g'(\theta)\neq 0$. Then following holds:

$$
\sqrt{n}(X_n-\theta) \overset{d}{\to} N(0, \sigma^2) \Rightarrow \sqrt{n}(g(X_n)-g(\theta)) \overset{d}{\to} N(0, \sigma^2(g'(\theta))^2)
$$\bigskip

\noindent This is evident because in $\sqrt{n}(g(X_n)-g(\theta))=\sqrt{n}g'(\theta)(X_n-\theta)+o(\sqrt{n}|X_n-\theta|)$, $|X_n-\theta|$ is bounded in probability by theorem 5.2.6 and thus $o(\sqrt{n}|X_n-\theta|) \overset{p}{\to} 0$ by theorem 5.2.8. As $\sqrt{n}(X_n-\theta) \overset{d}{\to} N(0,\sigma^2)$, $\sqrt{n}(g(X_n)-g(\theta)) \overset{d}{\to} N(0, \sigma^2(g'(\theta))^2)$ holds by Slutsky's theorem.
\end{document}

