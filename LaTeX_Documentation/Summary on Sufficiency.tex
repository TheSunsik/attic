\documentclass[10pt]{article}
\usepackage[utf8]{inputenc}
\usepackage{fullpage}
\usepackage{paralist}
\usepackage{amsmath}

\title{%
Summary for Sufficiency \\
\large{From Casella\ \&\ Berger textbook}}
\author{Sunsik Kim}
\date{}

\begin{document}
\maketitle

\noindent The observed sample $\textbf{x}=\{x_1,...,x_n\}$ is a long list of numbers that may be hard to interpret, so experimenter might wish to summarize the information in a sample by determining a few key features of the sample values, which is usually done by computing statistics. In fact, any statistic $T(\textbf{X})$ defines a form of a data reduction or data summary. Then, the natural question arises as: "Which statistic to use to perform good data reduction?". To make a reasonable criteria, there exists The Sufficiency Principle.

\section{How to obtain Sufficient Statistic}
A \textit{sufficient statistic} for a parameter $\theta$ captures all the information about $\theta$ contained in the sample. Specifically, this means that any inference about $\theta$ should depend on the sample $\textbf{X}=\{X_1,...,X_n\}$ only through the value $T(\textbf{X})$, which leads to the formal definition of:\bigskip

\textbf{Definition 6.2.1}\ \ \ A statistic statistic $T(\textbf{X})$ is a \textit{sufficient statistic for $\theta$} if the conditional distribution of the sample \textbf{X} given the value of $T(\textbf{X})=T(\textbf{x})$ does not depend on $\theta$.\bigskip

\noindent However, to use the definition, we must guess a statistic $T(\textbf{X})$ to be sufficient, find the pmf/pdf of $T(\textbf{X})$, and check that the ratio of the distributions. Fortunately, the next theorem allows us to find a sufficient statistic by simple inspection on the existence of a factorization of:\bigskip

\textbf{Theorem 6.2.6 (Factorization Theorem)} Let $f(\textbf{x}|\theta)$ denote the joint pdf or pmf of a sample \textbf{X}. A statistic $T(\textbf{X})$ is a \textit{sufficient statistic for $\theta$} if $f(\textbf{x}|\theta)$ can be factorized as
\begin{center}
    $f(\textbf{x}|\theta)=g(T(\textbf{x}|\theta)h(\textbf{x})$
\end{center}
By the Factorization Theorem, we only need to factor the joint pdf of the sample into two parts, with one part not depending on $\theta$.

\section{How to obtain Minimal Sufficient Statistic}
\noindent In fact, there are many sufficient statistics in any problem. To choose one statistic among such varieties, we again need certain criteria. We can think of a statistic that achieves the most data reduction as a reasonable choice. Definition of such a statistic is:\bigskip

\textbf{Definition 6.2.11} A sufficient statistic $T(\textbf{X})$ is called a \textit{minimal sufficient statistic} if, for any other sufficient statistic $T'(\textbf{X})$, $T(\textbf{X})$ is a function of $T'(\textbf{X})$.\bigskip

\noindent However, using the definition to find minimal sufficient statistic is again impractical. Fortunately, again, the following gives an easier way to find a minimal sufficient statistic.\bigskip

\textbf{Theorem 6.2.13 (Lehmann and Scheffe)} Let $f(\textbf{x}|\theta)$ be the joint pmf or pdf of a sample \textbf{X} where \textbf{x} and \textbf{y} refer to any sample points. Following holds:
\begin{center}
    $\displaystyle g(\theta)=\frac{f(\textbf{x}|\theta)}{f(\textbf{y}|\theta)}  \ \textrm{is a constant function iff}\ T(\textbf{x})=T(\textbf{y}) \rightarrow\ T(\textbf{X})\ \textrm{is a minimal sufficient statistic}$
\end{center}

\section{How to use Basu's Theorem}
\subsection{Ancillary statistic}
In this section, different sort of statistic is introduced, one that has a complementary purpose.\bigskip

\textbf{Definition 6.2.16} A statistic $S(\textbf{X})$ whose distribution does not depend on the parameter $\theta$ is called an \textit{ancillary statistic}.\bigskip

\noindent Unfortunately, there is no convenient way to show that a statistic is ancillary; we only need to show the one of the following:
\begin{compactenum}
    \item \textbf{Example 6.2.17} The distribution of the statistic $S(\textbf{X})$ does not rely on $\theta$.
    \item \textbf{Example 6.2.18} The distribution which belongs to the same location family does not rely on $\theta$
    \item \textbf{Example 6.2.19} The distribution which belongs to the same scale family does not rely on $\theta$
\end{compactenum}

\subsection{Complete Statistic}
Speaking of the family of a distribution, recall that the number of possible value of a parameter $\theta$ is infinite. Accordingly, the number of possible shape of a distribution is also infinite. So we call such set of a distribution $\{f(\textbf{x} |\theta ) \colon \theta \in \mathbf{A} \subset \mathbb{R} \} $ as a family of probability distributions. With that, we define complete statistic as following:\bigskip

\textbf{Definition 6.2.21} Let $f(t|\theta)$ be a family of pdfs or pmfs for a statistic $T(\textbf{X})$. The $T(\textbf{X})$ is called \textit{complete statistic} if setting $E_\theta g(T)=0$ for any $\theta$ only leads to the conclusion that $g(t)=0$.\bigskip

\noindent That is, to show that $T(\textbf{X})$ is a complete statistic, first obtain the pmf/pdf of $T(\textbf{X})$, and show that $E_\theta g(T)=0$ leads to $g(t)=0$. However, if \textbf{X} is iid sample from the member of exponential family, we can easily find out the complete statistic of $\theta$.

\textbf{Theorem 6.2.25} If \textbf{X} is iid sample from
\begin{center}
    $f(x|\boldsymbol{\theta})=h(x)c(\boldsymbol{\theta})exp(\displaystyle \sum_{j=1}^{k}w(\theta_{j})t_j(x))} $
\end{center}
\ \ \ \ where $\boldsymbol{\theta}=\{\theta_1, ..., \theta_k\}$, then the statistic(s)
\begin{center}
    $\{\sum_{i=1}^{n}t_1(X_i),...,\sum_{i=1}^{n}t_k(X_i)\}$
\end{center}
\ \ \ \ is complete statistic of \boldsymbol{\theta}.

\subsection{Basu's Theorem}
Complete statistic is nice property since it implies minimal sufficiency. That is:\bigskip

\textbf{Theorem 6.2.28} If a statistic $T(\textbf{X})$ is complete, then $T(\textbf{X})$ is minimal sufficient.\bigskip

\noindent Also, there exists a Basu's theorem that enhances the usage of complete statistic. This theorem is useful when showing the independence of two statistics without ever finding the joint distribution of the two statistics.\bigskip

\textbf{Theorem 6.2.24(Basu's Theorem)} If a statistic $T(\textbf{X})$ is complete, then it is independent of every ancillary statistic.\bigskip

\end{document}

